# Caching: Population and Scaling

## What is Caching?

Caching is a technique used to store frequently accessed data in a high-speed storage layer, reducing the need for repeated expensive computations or database queries.

## How to Populate a Cache

A cache typically sits between an API and a database, logically acting as an intermediary, even though it may be physically located elsewhere.

### Request Flow with Caching

1. **User makes a request** → API server receives it.
2. **API checks the cache** → If the data exists in the cache, return it.
3. **Cache miss occurs** → If the data is not in the cache:
   - API fetches data from the database.
   - API stores the fetched data in the cache.
   - API returns the data to the user.

## Lazy Population (Lazy Loading)

Lazy population is the most common caching strategy. Here, data is stored in the cache only when there is a cache miss.

### Steps in Lazy Population

1. API receives a request and checks the cache.
2. If the cache contains the data, it is returned.
3. If the cache does not contain the data, the API queries the database.
4. The retrieved data is stored in the cache.
5. The data is returned to the user.

### Example: Caching Blog Data

- Consider a blogging application that fetches blog details by joining multiple tables (e.g., `blogs`, `authors`, `tags`).
- Constructing the JSON response is expensive due to multiple table joins.
- Instead of querying the database every time, store the JSON response in the cache.
- Subsequent requests fetch data directly from the cache, reducing the load on the database.

### Importance of Expiry

- Every cached entry should have an expiration time (e.g., 5 min, 10 min, etc.).
- Prevents stale data from being served indefinitely.
- Reduces memory usage by automatically removing unused cache entries.
- Prevents memory leaks that could lead to performance degradation.

## Eager Population (Eager Loading)

Eager population proactively populates the cache before a request is made.

### Methods of Eager Population

#### 1. **Write-Through Caching (Database + Cache Write Simultaneously)**

- When data is written to the database, it is also written to the cache.
- Ensures that cached data is always up to date.
- **Example: Cricket Score Website**
  - A live cricket score website receives thousands of read requests.
  - Instead of waiting for the cache to expire before updating scores, the API updates the cache immediately when new scores are recorded.
  - Ensures that users always see the latest score without delays.

#### 2. **Preemptive Caching (Predictive Caching)**

- Data is proactively pushed into the cache based on expected future access patterns.
- **Example: Trending YouTube Videos**
  - When a trending video is recommended to users, it is likely to be accessed frequently.
  - YouTube preemptively caches metadata of trending videos.
  - Reduces cache misses and database queries.

## Scaling a Cache

Similar to databases, caches can be scaled using multiple strategies:

### 1. **Vertical Scaling**

- Increase hardware resources (RAM, CPU) to handle more cache data.
- Simple but has physical and cost limitations.

### 2. **Read Replicas**

- Similar to database replication.
- Multiple cache replicas serve read requests, reducing load on a single node.
- API servers distribute read requests among replicas.

### 3. **Distributed Caching (Sharding)**

- Data is partitioned across multiple cache nodes.
- Each node stores a mutually exclusive subset of data.
- API servers use a routing mechanism (e.g., hash-based, range-based) to fetch data from the correct shard.
- Each shard may have replicas for additional scaling.

## Conclusion

- **Lazy Population**: Only caches requested data, reducing unnecessary storage but increasing cache misses.
- **Eager Population**: Proactively caches anticipated data, improving performance but requiring careful management.
- **Scaling Strategies**: Vertical scaling, read replicas, and sharding help maintain cache efficiency as traffic grows.

By understanding these caching techniques and scaling methods, we can build efficient and scalable systems that minimize database load while ensuring fast response times.
