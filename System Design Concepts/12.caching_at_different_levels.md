# Caching in System Architecture

## Introduction

Caching is a powerful technique that significantly improves system performance by reducing latency and decreasing the load on backend systems. However, caching introduces challenges such as data staleness and cache invalidation. In this document, we explore different caching locations within a system architecture and discuss their advantages and trade-offs.

## Key Considerations in Caching

- **Performance Improvement**: Reduces database and server load by serving frequently requested data quickly.
- **Staleness**: Cached data may become outdated, leading to inconsistencies.
- **Invalidation**: Ensuring that outdated cache entries are removed or updated efficiently.
- **Use Case Specificity**: Some data (e.g., financial transactions) should not be cached, whereas other data (e.g., images, user preferences) can be safely cached.

## Common Caching Solutions

### 1. **Redis as a Cache**

Redis is one of the most widely used caching solutions due to its speed and versatility.

- **In-memory storage**: Ensures extremely fast read and write operations.
- **Supports expiration**: Keys can be set to expire, reducing stale data issues.
- **Supports eviction policies**: Can be configured to remove the least recently used (LRU) keys when memory is full.

However, Redis is not the only place where caching can be implemented.

## Caching Strategies Across System Layers

### 1. **Client-Side Caching**

Client-side caching stores data directly on the user's device (browser or mobile app), reducing server requests.

#### Use Cases:

- **Static resources**: Images, JavaScript files, and CSS stylesheets.
- **User session data:** Previously loaded information (e.g., recently watched videos on Amazon Prime Video).
- **LocalStorage or IndexedDB**: Storing less frequently changing data on the client.

#### Example:

When a user visits a website, the browser caches images and scripts. On subsequent visits, the page loads faster because these resources are retrieved from the local cache instead of making network requests.

### 2. **Content Delivery Network (CDN) Caching**

CDNs distribute cached content across geographically dispersed servers to improve content delivery speed.

#### How It Works:

1. A user requests a resource (e.g., an image or a video).
2. The request is routed to the nearest CDN server.
3. If the CDN has the resource in its cache, it serves it immediately.
4. If not, the CDN fetches the resource from the origin server, caches it, and serves it.

#### Use Cases:

- **Serving static assets (images, videos, CSS, JS files)**
- **Reducing latency for international users**
- **Handling high-traffic loads efficiently**

#### Example:

If a US user requests an image from a website hosted in India, fetching it from India introduces latency. A CDN caches the image on a US server, reducing the response time.

### 3. **Application-Level Caching**

Implemented within backend applications to reduce expensive database queries and computations.

#### Methods:

- **In-Memory Caching**: Using Redis or Memcached to store frequently accessed data.
- **Object Caching**: Storing serialized objects to avoid re-fetching or recalculating data.
- **Computed Results Caching**: Storing results of expensive calculations (e.g., frequently accessed analytics data).

#### Example:

Instead of querying a database for user profiles on each request, the application caches profiles in memory for a set duration.

### 4. **Database Caching**

Databases themselves implement caching mechanisms to improve query performance.

#### Techniques:

- **Query Result Caching**: Caching results of frequently executed queries.
- **Indexing**: Speeding up query performance by creating indexes.
- **Materialized Views**: Precomputing and storing query results for faster retrieval.

#### Example:

A database caching layer stores frequently accessed product details so that future queries return instantly.

### 5. **Proxy-Level Caching**

Reverse proxies like Nginx, Varnish, or Squid cache HTTP responses, reducing the load on application servers.

#### Use Cases:

- **Caching API responses**
- **Reducing backend server load**
- **Serving precomputed static pages**

#### Example:

If a user frequently requests the same API response, a reverse proxy can return the cached result instead of hitting the backend.

## Cache Invalidation Strategies

### 1. **Time-to-Live (TTL) Expiry**

Each cache entry is assigned an expiration time. Once expired, the cache automatically removes the entry.

### 2. **Write-Through Cache**

Data is written to both the cache and the database simultaneously to ensure consistency.

### 3. **Cache Eviction Policies**

- **Least Recently Used (LRU)**: Removes the least recently accessed items.
- **Least Frequently Used (LFU)**: Removes items accessed the least.
- **First-In-First-Out (FIFO)**: Removes the oldest entries first.

### 4. **Manual Invalidation**

Explicitly removing or updating cache entries when underlying data changes.

#### Example:

A banking application should manually invalidate the cache when a userâ€™s account balance updates to ensure accurate financial reporting.

## Conclusion

Caching is a critical technique to optimize system performance, but it comes with trade-offs. Understanding different caching strategies and their appropriate use cases helps in designing efficient, scalable, and reliable systems.

Here are steps to practice CDN and other caching strategies hands-on:

---

### **Practicing CDN**

**Goal:** Learn how to set up and use a CDN to cache and serve static content.

#### **Step 1: Choose a CDN Provider**

Popular CDN providers:

- [Cloudflare](https://www.cloudflare.com/)
- [AWS CloudFront](https://aws.amazon.com/cloudfront/)
- [Fastly](https://www.fastly.com/)
- [Akamai](https://www.akamai.com/)

#### **Step 2: Deploy a Static Website**

1. Create a simple HTML/CSS/JS website.
2. Deploy it on a cloud platform like AWS S3, GitHub Pages, or Vercel.

#### **Step 3: Configure CDN**

1. Sign up for a CDN provider.
2. Add your website's origin URL (e.g., your S3 bucket or Vercel domain).
3. Set up caching rules:
   - Cache static assets (images, CSS, JS).
   - Configure TTL (Time-to-Live).
4. Enable compression (Gzip/Brotli) to optimize performance.

#### **Step 4: Test CDN Performance**

- Use `curl -I <CDN_URL>` to check headers like `X-Cache: HIT`.
- Compare response time before and after using CDN.
- Use [GTmetrix](https://gtmetrix.com/) or Chrome DevTools to analyze caching behavior.

---

### **Practicing Redis Caching**

**Goal:** Implement caching for API responses using Redis.

#### **Step 1: Install Redis**

```bash
sudo apt update && sudo apt install redis -y
```

Start Redis:

```bash
redis-server
```

#### **Step 2: Connect to Redis in Python**

Install Redis client:

```bash
pip install redis
```

Simple Python script:

```python
import redis

r = redis.Redis(host='localhost', port=6379, db=0)

# Set cache
r.set("username", "Deepak")

# Get cache
print(r.get("username").decode("utf-8"))  # Output: Deepak
```

#### **Step 3: Cache API Responses**

```python
import redis
import requests

r = redis.Redis(host='localhost', port=6379, db=0)

def get_weather(city):
    cache_key = f"weather:{city}"
    if r.exists(cache_key):
        print("Fetching from cache...")
        return r.get(cache_key).decode("utf-8")

    print("Fetching from API...")
    response = requests.get(f"https://api.weatherapi.com/v1/current.json?key=YOUR_API_KEY&q={city}")
    data = response.json()

    # Cache the response for 5 minutes
    r.setex(cache_key, 300, str(data))

    return data

print(get_weather("New York"))  # First time fetches from API
print(get_weather("New York"))  # Second time fetches from cache
```

---

### **Practicing Reverse Proxy Caching (Nginx)**

**Goal:** Use Nginx to cache API responses.

#### **Step 1: Install Nginx**

```bash
sudo apt install nginx -y
```

#### **Step 2: Configure Nginx as a Reverse Proxy**

Edit Nginx configuration:

```bash
sudo nano /etc/nginx/sites-available/default
```

Add caching rules:

```nginx
server {
    listen 80;
    server_name example.com;

    location /api/ {
        proxy_pass http://your-backend-server;
        proxy_cache_path /var/cache/nginx levels=1:2 keys_zone=my_cache:10m inactive=60m;
        proxy_cache my_cache;
        proxy_cache_valid 200 302 60m;
        proxy_cache_valid 404 1m;
    }
}
```

#### **Step 3: Restart Nginx**

```bash
sudo systemctl restart nginx
```

Test:

```bash
curl -I http://example.com/api/
```

Check for `X-Cache: HIT` in response headers.

---
