# Load Balancers

Load balancers are one of the most important infrastructure components in modern systems. They enable horizontal scalability and ensure efficient distribution of incoming traffic to backend servers. We often take load balancers for granted because, when we think about scaling an API server, we immediately consider placing it behind a load balancer.

Let's dive deep into the concept of load balancers, their features, and different load-balancing algorithms.

---

## What is a Load Balancer?

A **load balancer** is a network component that acts as the only point of contact between clients and backend servers. It efficiently distributes incoming traffic among multiple backend servers to optimize resource utilization, minimize response time, and avoid server overload.

### Why is a Load Balancer Needed?

Without a load balancer:

- Clients must be aware of individual backend server IPs.
- Clients must decide which server to send requests to.
- Scaling becomes complex as new servers are added.

With a load balancer:

- Clients only need to know a single IP address or domain name.
- The load balancer distributes requests to available backend servers.
- The system can scale horizontally without requiring changes on the client side.

---

## How Does a Load Balancer Work?

### Request Flow

1. The client requests a resource using the load balancer’s IP address or domain name.
2. The request reaches the load balancer.
3. The load balancer selects a backend server based on a load-balancing algorithm.
4. The request is forwarded to the chosen server.
5. The backend server processes the request and sends the response to the load balancer.
6. The load balancer forwards the response to the client.

### Load Balancers for Service-to-Service Communication

- Load balancers are not only used for human users but also for inter-service communication.
- Example: An authentication service calling a profile service will route requests through the profile service's load balancer.

---

## Load Balancing Algorithms

Load balancers use different algorithms to distribute traffic among backend servers. Let's explore the most common ones:

### 1. Round Robin

- Requests are distributed sequentially to backend servers in a cyclic manner.
- Example:
  - Request 1 → Server 1
  - Request 2 → Server 2
  - Request 3 → Server 3
  - Request 4 → Server 1 (repeats the cycle)
- **Use Case:** When all backend servers have equal capacity.

### 2. Weighted Round Robin

- Similar to Round Robin but assigns weights to servers based on their capacity.
- Example:
  - Server 1 (4GB RAM) → Weight = 1
  - Server 2 (8GB RAM) → Weight = 2
  - Server 3 (4GB RAM) → Weight = 1
  - Requests are distributed in a 1:2:1 ratio.
- **Use Case:** When servers have different processing power.

### 3. Least Connections

- The request is sent to the server with the least number of active connections.
- Helps prevent overload on servers processing long-running requests.
- **Use Case:** When request processing time varies significantly (e.g., some requests take 1 second while others take minutes).

### 4. Hash-Based Routing

- The load balancer selects a server based on a hash of request parameters (e.g., client IP, session ID, etc.).
- Ensures that requests from the same client are routed to the same backend server (session persistence).
- **Use Case:** When session persistence is required, such as in stateful applications.

---

## Conclusion

Load balancers are essential for building scalable and resilient systems. They provide a single point of contact, abstract backend complexity, and ensure efficient traffic distribution. The choice of a load balancing algorithm depends on factors such as server capacity, request variability, and session persistence requirements.
